{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Radar classification practicum 1 - Classification models_no_answers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KB4FZPfDxJms",
        "3JYwhi0Nzwbb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR2b0_iH_qN9"
      },
      "source": [
        "Author: Peter Svenningsson\n",
        "\n",
        "Email: p.o.svenningsson@tudelft.nl\n",
        "\n",
        "Updated: 2021-04-24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4FZPfDxJms"
      },
      "source": [
        "# Welcome!\n",
        "This is a Jypter notebook which is hosted on Google colab.\n",
        "Here we can run Python code based on the interactive python (Ipython) implementation of the python language\n",
        "\n",
        "Code is written in cell blocks which are run by pressing the arrow symbol in the top right of the cell. Any variables and functions which are defined are stored in your namespace (workspace) much like in Matlab. \n",
        "\n",
        "To clear the namespace, to go Runtime -> Restart runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JYwhi0Nzwbb"
      },
      "source": [
        "# Dataset and classification task\n",
        "The data has been recorded using a multi-static coherent pulsed radar configuration with three radar nodes. In this practicum we will use the data recorded by a node which transmits and recieves in V polarization. The waveform is a linearly modulated up-chip with bandwidth $45$ MHz, $18$ dBi gain, $23$ dBm transmit power and PRF $5$ kHz\n",
        "\n",
        "The radar measures the micro-Doppler signature of a person walking towards the sensor. The classes are defined as\n",
        "\n",
        "\n",
        "*   Walking\n",
        "*   Walking while carrying a heavy backpack\n",
        "*   Walking while carring a metal pole\n",
        "\n",
        "The micro-Doppler signature of a three second sample is characterized by the mean and variance of the Doppler centroid and the Doppler bandwidth as you have discussed in the lectures.\n",
        "\n",
        "The task is then to use some classification method to create optimal decision boundaries which seperates the classes and has strong predictive performance on new data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4BF21us2330"
      },
      "source": [
        "# Tabular data formats\n",
        "In Matlab tabular data is often stored in an array.\n",
        "\n",
        "In Python and R it is popular to store tabular data in a dataframe. You can think of a dataframe as an array with additional functionality. One of the main differences in working with a dataframe is that the columns of the array are not retrieved by an index. Rather they are retrieved by the column name. This is handy because we not have to keep track of what quantities the different columns signify.\n",
        "\n",
        "Run the cell below to load the dataframe and print the first five samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koVqc-qutlD-"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve('https://github.com/petersvenningsson/student-resources-EE4675/blob/main/NetRad_dataframe?raw=true', 'NetRad_dataframe')\n",
        "dataset_dataframe = pickle.load(open( 'NetRad_dataframe', 'rb' ))\n",
        "\n",
        "dataset_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygH1e1tsdjhX"
      },
      "source": [
        "\n",
        "\n",
        "<br> <br> <br> \n",
        "\n",
        "\n",
        "# Extracting data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQVW2ol4Ew1Y"
      },
      "source": [
        "We can extract a column as a numpy array using the column name as a key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF5fq9-bcrsc"
      },
      "source": [
        "bandwidth_mean = dataset_dataframe['Bandwidth mean'].to_numpy()\n",
        "labels = dataset_dataframe['Class index'].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPvXpxysdtBz"
      },
      "source": [
        "We can also extract the complete feature array using a list of the relevant column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86ZATWFAd35s",
        "outputId": "2761ee18-f361-4412-a21d-9f487013f2ed"
      },
      "source": [
        "feature_columns = ['Bandwidth mean', 'Bandwidth std', 'Centroid mean', 'Centroid std']\n",
        "feature_array = dataset_dataframe[feature_columns].to_numpy()\n",
        "print(f\"The shape of the feature array is {feature_array.shape[0]} rows and {feature_array.shape[1]} columns\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the feature array is 360 rows and 4 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqsHXaFienMa"
      },
      "source": [
        "Tthe spectrograms have been stored in the *Spectrogram* column. Lets define a function plot_spectrogram() which plots a spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq-rFz5ge9Vk"
      },
      "source": [
        "def plot_spectrogram(sample_index):\n",
        "    spectrogram = dataset_dataframe['Spectrogram'][sample_index]\n",
        "    plt.imshow(spectrogram, aspect='auto', cmap='jet', origin='lower', vmin=-40, interpolation='Nearest')\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.ylabel('Doppler bin')\n",
        "    plt.xlabel('Time bin')\n",
        "    plt.title(f'''The class is {dataset_dataframe[\"Class\"][sample_index]}.\n",
        "        The feature Centroid std is {round(dataset_dataframe[\"Centroid std\"].to_numpy()[sample_index],3)}''')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_spectrogram(sample_index = 0)\n",
        "plot_spectrogram(sample_index = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btwk3h8Nn-WB"
      },
      "source": [
        "\n",
        "\n",
        "<br> <br> <br> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Exercise 1: Hold-out validation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In predictive modelling, the usefulness of a classification model is determined by its performance on new data. Therefore the dataset is split into a training set and a test set. The model is fit to the training set and its performance is evaluate on the test set.\n",
        "\n",
        "\n",
        "**Implement the function get_splits()**\n",
        "\n",
        "The function get_splits() should\n",
        "1.   Return  the indices of the training set and the test set as two lists of integers.\n",
        "2.   The training and test sets should be disjoint\n",
        "3. The union of the training and test sets should be equal to the set of all samples \n",
        "4. The training set should consists of 80 % of all samples\n",
        "\n",
        "A experimental dataset typically has temporal correlations. For example, in this dataset the samples 0 to 89 are recorded from *Person 1* and samples 90 to 179 are recorded from *Person 2*. To minimize the impact of these correlations, the training and test indices should be selected randomly from the dataset.\n",
        "\n",
        "Reference functions: random.shuffle()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs9cMp5aj4MM"
      },
      "source": [
        "import random\n",
        "\n",
        "def get_splits(dataset_dataframe):\n",
        "    \"\"\" Returns training and validation set indices.\n",
        "\n",
        "        training_indices: List of integers\n",
        "        test_indices: List of integers\n",
        "    \"\"\"\n",
        "    # Your code\n",
        "\n",
        "    return training_indices, test_indices\n",
        "\n",
        "\n",
        "#########\n",
        "# TESTS # \n",
        "#########\n",
        "\n",
        "training_indices, test_indices = get_splits(dataset_dataframe)\n",
        "n_samples = len(dataset_dataframe)\n",
        "\n",
        "assert isinstance(training_indices, list) and isinstance(test_indices, list),\\\n",
        "    'training_indices and test_indices should be Lists'\n",
        "assert len(training_indices) == int(0.8*n_samples),\\\n",
        "    'The training indices have an incorrect length'\n",
        "assert len(test_indices) == int(0.2*n_samples),\\\n",
        "    'The test indices have an incorrect length'\n",
        "assert set(training_indices).intersection(set(test_indices)) == set(),\\\n",
        "    'The training and test sets are not disjoint'\n",
        "assert set(training_indices).union(set(test_indices)) == set(range(n_samples)),\\\n",
        "    'Some samples are neither in the training set or test set'\n",
        "assert sum(training_indices) != sum(range(int(n_samples*0.8))),\\\n",
        "    'The training set is ordered 0 through 288. This is incorrect - please select the training and test indices randomly'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma_lbqaS8dyd"
      },
      "source": [
        "<br> <br> <br> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Exercise 2: Data standardization\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "It is good practice to standardize the feature data. This entails translating the data so that the mean of each feature is equal to zero and scaling the data so that the standard deviation of each feature is one. Standardization is particularly important when working with distance based classifiers such as support-vector machines or K-nearest neighbours. This is because we need the feature dimensions to be in some sense directly comparable when working with distance metrics. Standardization is also important when working with principle component analysis (PCA) as otherwise features with high variance may domainate.\n",
        "\n",
        "For each feature $x_i$\n",
        "\n",
        "1.   Estimate the mean $\\mu$ and standard deviation $\\sigma$\n",
        "2.   $x_i \\leftarrow \\frac{x_i-\\mu}{\\sigma}$\n",
        "\n",
        "The mean $\\mu$ and standard deviation $\\sigma$ are estimated from the training set and the transformation $x_i = \\frac{x_i-\\mu}{\\sigma}$ is applied to both the training set and the test set.\n",
        "\n",
        "**Implement the function standardize_features()**\n",
        "\n",
        "Reference functions:\n",
        "np.mean()\n",
        "np.std()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqx_SXoyFxg9"
      },
      "source": [
        "def standardize_features(feature_array, training_indices):\n",
        "    \"\"\" Standardizes the feature array based on the moments estimated\n",
        "        from the training split.\n",
        "\n",
        "        normalized_feature_array: np.array of shape (n_samples, n_features)\n",
        "    \"\"\"\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "    return normalized_array\n",
        "\n",
        "\n",
        "#########\n",
        "# TESTS # \n",
        "#########\n",
        "feature_array = dataset_dataframe[feature_columns].to_numpy()\n",
        "\n",
        "training_indices, test_indices = get_splits(dataset_dataframe)\n",
        "normalized_array = standardize_features(feature_array, training_indices)\n",
        "\n",
        "assert np.sum(np.mean(normalized_array[training_indices,:], axis=0)) < 1e-8,\\\n",
        "    'The training split feature means are not close to zero'\n",
        "assert np.std(np.std(normalized_array[training_indices,:], axis=0) - 1) < 1e-8,\\\n",
        "    'The training split feature standard deviations are not close to one'\n",
        "\n",
        "assert np.sum(np.mean(normalized_array[test_indices,:], axis=0)) < 0.5,\\\n",
        "    'The test split feature means are not close to zero'\n",
        "assert np.std(np.std(normalized_array[test_indices,:], axis=0) - 1) < 0.5,\\\n",
        "    'The test split feature standard deviations are not close to one'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQrIG8ScMWP3"
      },
      "source": [
        "<br> <br> <br> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Exercise 3: Performance metrics: Macro-, micro-average \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Many performance metrics such as recall, precision and F scores are defined for binary classification. Two alternativs to extend these metrics for the multi-class setting are micro and macro averaging. \n",
        "\n",
        "The performance metrics are typically defined as functions of statistics such as True Positives (TP) and false negatives (FN).\n",
        "*   Macro-average: The performance metric is calculated for each class and the mean of these is taken as the macro-averaged metric.\n",
        "*   Micro-average: A statistic (such as TP) are defined by the sum of the  statistic for each class.\n",
        "\n",
        "<br>\n",
        "\n",
        "For example, in the binary setting we define,\n",
        "\n",
        "> $\\text { Recall}=\\frac{TP}{TP+FN}.$\n",
        "\n",
        "\n",
        "\n",
        "For $C$ classes we can calculate the recall of each class $\\text { Recall}_i,\\quad i \\in 1,\\dots, C$\n",
        "\n",
        "and the statistics for each class $TP_i, TN_i\\,\\quad i \\in 1,\\dots, C$\n",
        "\n",
        "<br>\n",
        "\n",
        "The micro-averaged recall is then defined as,\n",
        "\n",
        "\n",
        "> $\\text{ Recall}_{\\text{mi}}=\\frac{\\sum_i^C TP_i}{\\sum_i^C TP_i+\\sum_i^C FN_i}.$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The macro-averaged recall is then defined as,\n",
        "\n",
        "\n",
        "\n",
        "> $\\text{ Recall}_{\\text{ma}}=\\frac{1}{C}\\sum_i \\text { Recall}_i$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "*   The macro-averaged recall is sometimes referred to as balanced accuracy and it may be appropriate to use when there are large class-imbalances in the dataset. \n",
        "\n",
        "*   Micro-averaged recall is equal to the total accuray.\n",
        "\n",
        "**Implement the function calculate_recall()**\n",
        "\n",
        "Reference functions: sum(), max(), len()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp5dExZyJoFk"
      },
      "source": [
        "def calculate_recall(labels, predictions, averaging_type = 'macro'):\n",
        "    \"\"\" Calculates and returns the recall metric in the multi-class setting.\n",
        "\n",
        "        predictions: List or 1D numpy array of integers\n",
        "        labels: List or 1D numpy array of integers\n",
        "    \"\"\"\n",
        "\n",
        "    if averaging_type == 'macro':\n",
        "    \n",
        "        # Your code\n",
        "\n",
        "        return recall\n",
        "\n",
        "    if averaging_type == 'micro':\n",
        "\n",
        "        # Your code\n",
        "\n",
        "        return recall\n",
        "        \n",
        "\n",
        "#########\n",
        "# TESTS # \n",
        "#########\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "predictions = np.array([0,1,1,1,1,1,1,1,1,1,2])\n",
        "labels = np.array([0,1,1,1,1,1,1,1,1,2,0])\n",
        "\n",
        "assert np.abs(calculate_recall(labels, predictions, 'macro') - \n",
        "    recall_score(labels, predictions, average ='macro')) < 1e-2,\\\n",
        "    'The macro-averaging implementation is incorrect'  \n",
        "\n",
        "assert np.abs(calculate_recall(labels, predictions, 'micro') -\n",
        "    recall_score(labels, predictions, average ='micro')) < 1e-2, \\\n",
        "    'The micro-averaging implementation is incorrect'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmlcKDT3jDHd"
      },
      "source": [
        "<br> <br> <br> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Exercise 4: Classification model - Logistic regression \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "One of the most simple classification models is logistic regression. A logistic regression model gets its name from that a linear model predicts the log-odds of a sample beloning to a specific class. Prediction of continous values like log-odds are often called regression tasks. The log-odds are named logits and are here denoted by $\\underset{1 \\times C}{\\boldsymbol{z}}=\\underset{1 \\times k}{\\boldsymbol{x}} \\underset{\\, \\,k \\times C}{\\boldsymbol{W}}$, predicted by the weights $\\boldsymbol{W}$ and $k$ features $\\boldsymbol{x}$ for $C$ classes.\n",
        "\n",
        "Since we are interested in classification, we exponentiate the logits and normalize the odds by dividing by their sum. This operation is named the softmax function,\n",
        "\n",
        "> $P(Y=y_c) = \\operatorname{softmax}(z_c)=\\frac{e^{z_c}}{\\sum_{i=1}^{k} e^{z_{i}}}$\n",
        "\n",
        "where class is indicated by the $Y$ variable.\n",
        "\n",
        "The class LogisticRegression implements a logistic regression classification model with multi-class support. The fit() method fits the model to a cross-entropy loss using gradient descent.\n",
        "\n",
        "\n",
        "The input to the fit() and predict() methods is the feature array X, sometimes also referred to as the design matrix,\n",
        "\n",
        "\n",
        "> $X=\\left[\\begin{array}{c}\\boldsymbol{x}_{1} \\\\\n",
        "\\boldsymbol{x}_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\boldsymbol{x}_{n}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{ccccc}\n",
        "x_{11} & x_{12} & x_{13} & \\ldots & x_{1 k} \\\\\n",
        "x_{21} & x_{22} & x_{23} & \\ldots & x_{2 k} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{m 1} & x_{n 2} & x_{n 3} & \\ldots & x_{n k}\n",
        "\\end{array}\\right]$, for $n$ data samples.\n",
        "\n",
        "\n",
        "**Implement the methods: logits, softmax and predict()**\n",
        "\n",
        "The predict() method should:\n",
        "\n",
        "\n",
        "1.   Calculate the logits by calling the calculate_logits() method\n",
        "2.   Calculate class probabilities by calling the softmax() method\n",
        "3.   Select the most probable class as the prediction and return the predictions as a 1D array of integers\n",
        "\n",
        "\n",
        "\n",
        "Reference functions: Matrix multiplication X @ W, np.exp(), np.argmax()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViIVX7mixXpS"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self):\n",
        "        \"\"\" Initializes the weights W.\n",
        "            The weights may be accessed by self.W\n",
        "        \"\"\"\n",
        "        self.W = None\n",
        "\n",
        "    def calculate_logits(self, X, W):\n",
        "        \"\"\" Returns the logits given the features in X and weights in W.\n",
        "\n",
        "            X: Numpy array shape (n_samples, n_features)\n",
        "            W: Numpy array shape (n_features, n_classes)\n",
        "            logits: Numpy array (n_samples, n_classes)\n",
        "        \"\"\"\n",
        "        # Your code here\n",
        "    \n",
        "        return logits\n",
        "\n",
        "    def softmax(self, logits):\n",
        "        \"\"\" Evaluates the softmax function on the rows of logits\n",
        "\n",
        "            logits: Numpy array shape (n_samples, n_classes)\n",
        "            probabilities: Numpy array shape (n_samples, n_classes)\n",
        "        \"\"\"\n",
        "        # Your code here\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Calls the methods self.calculate_logits and self.softmax to \n",
        "        generate the class predictions for the samples in feature array X.\n",
        "\n",
        "            predictions: Numpy array of integers shape (n_samples,)\n",
        "        \"\"\"\n",
        "\n",
        "        W = self.W\n",
        "        # You code here\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\" Calls the methods self.calculate_logits and self.softmax to \n",
        "        generate predicted class distributions for the samples in feature array X.\n",
        "\n",
        "            probabilities: Numpy array of integers shape (n_samples, n_classes)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Your code here\n",
        "        pass\n",
        "        return probabilities\n",
        "\n",
        "    \n",
        "    def loss(self, X, Y_onehot, W):\n",
        "        \"\"\" Calculates and returns the mean negative log likelihood\n",
        "        \"\"\"\n",
        "        logits = self.calculate_logits(X, W)\n",
        "        n_samples = X.shape[0]\n",
        "        loss = 1/n_samples * (np.trace(X @ W @ Y_onehot.T) + \n",
        "                              np.sum(np.log(np.sum(np.exp(logits), axis=1))))\n",
        "        return loss\n",
        "\n",
        "    def gradient(self, X, Y_onehot, W):\n",
        "        \"\"\" Calculates the gradient of the mean negative log likelihood\n",
        "        \"\"\"\n",
        "        logits = self.calculate_logits(X, W)\n",
        "        probabilities = self.softmax(logits)\n",
        "        n_samples = X.shape[0]\n",
        "        gradient = -1/n_samples * (X.T @ (Y_onehot - probabilities)) \n",
        "        return gradient\n",
        "\n",
        "    def gradient_descent(self, X, Y, max_iter=10000, learning_rate=0.1):\n",
        "        \"\"\" Fits the weights W by max_iter iterations of gradient descent.\n",
        "        \"\"\"\n",
        "        onehot_encoder = OneHotEncoder(sparse=False)\n",
        "        Y_onehot = onehot_encoder.fit_transform(Y.reshape(-1,1))\n",
        "        W = np.zeros((X.shape[1], Y_onehot.shape[1]))\n",
        "    \n",
        "        for _ in range(max_iter):\n",
        "            W = W - learning_rate * self.gradient(X, Y_onehot, W)\n",
        "\n",
        "        return W\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        self.W = self.gradient_descent(X, Y)\n",
        "\n",
        "\n",
        "#########\n",
        "# TESTS # \n",
        "#########\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "W_tmp = np.array([[0, 0], [1, 0], [0, 1]])\n",
        "X_tmp = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "logits = model.calculate_logits(X_tmp, W_tmp)\n",
        "assert logits.shape == (3, 2), 'The dimensions of the logits array are incorrect'\n",
        "assert np.all(logits == np.array([[2, 3], [5, 6], [8, 9]])),\\\n",
        "    'The LogisticRegression.calculate_logits() method is incorrect.'\n",
        "\n",
        "probabilities = model.softmax(logits)\n",
        "assert probabilities.shape == (3, 2), ('The dimensions of the probabilities',\n",
        "    'array are incorrect')\n",
        "assert all(np.abs((np.sum(probabilities, axis = 1) - 1)) < 1e-5),\\\n",
        "    'The normalized probabilities does not sum to 1.'\n",
        "\n",
        "model.W = W_tmp\n",
        "predictions = model.predict(X_tmp)\n",
        "assert predictions.shape == (3,), 'The dimensions of the predicions array are incorect'\n",
        "assert all(predictions == 1), 'The predictions are incorrect.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVH9wUEBxek1"
      },
      "source": [
        "<br> <br> <br> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Exercise 5: Classification pipeline\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "In exercise 1-3 we have implemented all the steps needed to generate predictions from a classification model with holdout validation. \n",
        "\n",
        "\n",
        "**Fit and evaluate the model LogisticRegression with holdout validation**\n",
        "\n",
        "\n",
        "1.   Split the dataset into a training and test set\n",
        "2.   Fit the model on the training set\n",
        "3.   Evaluate the model on the test set using the metric macro-averaged recall\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjI5IFEUzzev"
      },
      "source": [
        "training_indices, test_indices = get_splits(dataset_dataframe)\n",
        "\n",
        "feature_columns = ['Bandwidth mean', 'Bandwidth std', 'Centroid mean', 'Centroid std']\n",
        "feature_array = dataset_dataframe[feature_columns].to_numpy()\n",
        "feature_array = standardize_features(feature_array, training_indices)\n",
        "labels = dataset_dataframe['Class index'].to_numpy()\n",
        "\n",
        "model.fit(feature_array[training_indices], labels[training_indices])\n",
        "\n",
        "# Your code here\n",
        "\n",
        "print('The balanced accuracy is {:.2%}'.format(macro_recall))\n",
        "\n",
        "\n",
        "#########\n",
        "# TESTS # \n",
        "#########\n",
        "\n",
        "assert macro_recall < 0.3, ('Something is incorrect, the model performs worse '\n",
        "    'than random selection')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFc4fNeY2Qul"
      },
      "source": [
        "## Exercise 5: Questions\n",
        "\n",
        "### 5.1\n",
        "Try to run exercise 5 with and without the standardization of the features. Does feature standardization impact the performance of the model? If so, why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch0cbSZu3aGW"
      },
      "source": [
        "Student's answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwU3qwUp3aO-"
      },
      "source": [
        "### 5.2\n",
        "The training and test splits are randomly generated. Try to run exercise 5. several times. Does the performance change? If so, how should you characterize the performance of the model? What is the maximum and minimum performance you are able to acheive?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqdS0kes3aun"
      },
      "source": [
        "Student's answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20y-gNM379Jl"
      },
      "source": [
        "<br> <br> <br> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Exercise 6: Evaluating classification models from scikit-learn\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The LinearRegression class created in exercise 4 implements the same interface which is used by the classification models in the popular machine learning library *scikit-learn*. Namely the methods fit() and predict().\n",
        "\n",
        "**Fit and evaluate classification models from scikit-learn with holdout validation**\n",
        "\n",
        "\n",
        "*   Use the functions you have implemented above to evaluate the support-vector machine, naive Bayes and k-nearest neighbors on the provided dataset of micro-Doppler signatures\n",
        "*   Evaluate the performance with and without feature standardization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OGmgU3k8mrK"
      },
      "source": [
        "from sklearn.svm import SVC as SupportVectorMachine\n",
        "from sklearn.naive_bayes import GaussianNB as GaussianNaiveBayes\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNearestNeighbors\n",
        "\n",
        "model_SVM = SupportVectorMachine(kernel = 'rbf')\n",
        "model_NB = GaussianNaiveBayes()\n",
        "model_NN = KNearestNeighbors()\n",
        "\n",
        "# Your code here\n",
        "\n",
        "print('The balanced accuracy is {:.2%}'.format(macro_recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWTt6fB-AQl4"
      },
      "source": [
        "# Exercise 6: Questions\n",
        "### 6.1\n",
        "Evaluate the models with and without feature standardization. Which models seem to benifit from standardization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7PDQ-SWAjxS"
      },
      "source": [
        "Student's answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHSyeQhFBZEM"
      },
      "source": [
        "<br> <br> <br> \n",
        "\n",
        "\n",
        "\n",
        "# Non-obligatory exercise: Sensor fusion\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "As previously stated, the dataset has been collected using a multi-static radar network.\n",
        "![](https://raw.githubusercontent.com/petersvenningsson/student-resources-EE4675/1daf71f41f1abf6ab8a45732b04e3ad36a27f2ed/radar_configuration.svg)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lLuoBtEUo_w"
      },
      "source": [
        "# Load and visualize the tabular data containing Node 1, Node 2 and Node 3\n",
        "urllib.request.urlretrieve('https://github.com/petersvenningsson/student-resources-EE4675/blob/main/NetRad_dataframe_fusion?raw=true', 'NetRad_dataframe')\n",
        "fusion_dataframe = pickle.load(open( 'NetRad_dataframe', 'rb' ))\n",
        "fusion_dataframe[358:363]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktHMge1BUpHJ"
      },
      "source": [
        "Until now we have only worked with the data from the monostatic *Node 1*. It may be beneficial to fuse the class-predictions from the three sensing nodes. One way to fuse predictions is by taking the mean of the three predicted class  probability distributions. \n",
        "\n",
        "\n",
        "**Implement sensor fusion**\n",
        "1.   Implement the predict_proba() method in the LogisticRegression Class\n",
        "2.   Fit one classification model for each sensing node\n",
        "3.   Fuse the predictions on the test set and evaluate the results\n",
        "\n",
        "Reference functions: np.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jcF1gHOQuMO"
      },
      "source": [
        "dataframe_node_1 = fusion_dataframe.loc[fusion_dataframe.Node == 'Node 1']\n",
        "dataframe_node_2 = fusion_dataframe.loc[fusion_dataframe.Node == 'Node 2']\n",
        "dataframe_node_3 = fusion_dataframe.loc[fusion_dataframe.Node == 'Node 3']\n",
        "\n",
        "training_indices, test_indices = get_splits(dataframe_node_1) # This split is coherent for the three nodes\n",
        "labels = dataframe_node_1['Class index'].to_numpy() # These labels are coherent for the three nodes\n",
        "# Your code here\n",
        "\n",
        "print(f'The recall for the fused prediction is {round(fused_recall,3)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}