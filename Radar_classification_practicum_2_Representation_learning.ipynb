{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Radar classification practicum 2 - Representation learning  - no answers.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR2b0_iH_qN9"
      },
      "source": [
        "Author: Peter Svenningsson\n",
        "\n",
        "Email: p.o.svenningsson@tudelft.nl\n",
        "\n",
        "Updated: 2021-04-28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKbW1SQkyKuO"
      },
      "source": [
        "# Representation learning\n",
        "\n",
        "In radar practicum 1 we explored classification where a spectrogram was characterized by a set of features which we had defined. In challanging classification tasks it may be  difficult to engineer features which sufficiently separates the classes in the feature space.\n",
        "\n",
        "If we have a large dataset available we may instead try to learn features directly from the data and in this practicum we will cover the two most popular approaches:\n",
        "\n",
        "*   Deep neural networks\n",
        "*   Principal Component Analysis (PCA)\n",
        "\n",
        "<br>\n",
        "\n",
        "We will be working with the dataset described in radar practicum 1 - which is downloaded by the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b71BndP_-l5"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve('https://github.com/petersvenningsson/student-resources-EE4675/blob/main/NetRad_dataframe?raw=true', 'NetRad_dataframe')\n",
        "dataset_dataframe = pickle.load(open( 'NetRad_dataframe', 'rb' ))\n",
        "\n",
        "dataset_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3tM7hlS1P6I"
      },
      "source": [
        "Please copy to this file the following functions which you implemented in classification practicum 1: \n",
        "\n",
        "*get_splits*, *standardize_features*, *calculate_recall*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_op8ZCcVJuYo"
      },
      "source": [
        "import random\n",
        "\n",
        "def get_splits(dataset_dataframe):\n",
        "    \"\"\" Returns training and validation set indices.\n",
        "\n",
        "        training_indices: List of integers\n",
        "        test_indices: List of integers\n",
        "    \"\"\"\n",
        "    # Your code\n",
        "\n",
        "    # Answer\n",
        "\n",
        "    indices = list( range( len(dataset_dataframe) ) )\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    n_samples = len(dataset_dataframe)\n",
        "\n",
        "    training_indices = indices[0:int(n_samples*0.8)]\n",
        "    test_indices = indices[int(n_samples*0.8):]\n",
        "    return training_indices, test_indices\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5VzGD5kJxoW"
      },
      "source": [
        "def standardize_features(feature_array, training_indices):\n",
        "    \"\"\" Standardizes the feature array based on the moments estimated\n",
        "        from the training split.\n",
        "\n",
        "        normalized_feature_array: np.array of shape (n_samples, n_features)\n",
        "    \"\"\"\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "    return normalized_array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZBtBSNc1OQI"
      },
      "source": [
        "def calculate_recall(labels, predictions, averaging_type = 'macro'):\n",
        "    \"\"\" Calculates and returns the recall metric in the multi-class setting.\n",
        "\n",
        "        predictions: List or 1D numpy array of integers\n",
        "        labels: List or 1D numpy array of integers\n",
        "    \"\"\"\n",
        "    # Cast labels and predictions to numpy arrays\n",
        "    labels = np.array(labels)\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    if averaging_type == 'macro':\n",
        "    \n",
        "        # Your code\n",
        "\n",
        "        return recall\n",
        "\n",
        "    if averaging_type == 'micro':\n",
        "\n",
        "        # Your code\n",
        "\n",
        "        return recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanU4ay81q9x"
      },
      "source": [
        "# Exercise 1: Artificial neural networks\n",
        "A deep neural network is a compound function of parametrized linear functions (neurons) and non-linear activation functions. The parameters of the linear functions are optimized with respect to a loss function which acts as a suitable proxy task for the task we want to complete. \n",
        "\n",
        "For classification tasks a common loss function is the negative log likelihood defined as, $-\\sum_{i=j}^{C} y_{j} \\log \\hat{p}_{j}$, where $y_j \\in \\{0,1\\}$ denotes the true label and $\\hat{p}_{j}$ the predicted probability of the sample belonging to class $j$. This loss function is differentiable which allows us to use gradient based optimizers like stochastic gradient descent to fit the neural network to training data.\n",
        "\n",
        "The input to the neural network is first passed through a set of neurons which maps the input to a high dimensional feature space. This feature space is the learnt representation where classes should be well separated. The last layer of the neural network consists simply of a logistic regression model which takes as input the learnt representation. \n",
        "\n",
        "Run the two cells below to train a fully connected deep neural network to classify the previously discussed micro-Doppler signatures.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-941tUE8czw"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "\n",
        "training_indices, test_indices = get_splits(dataset_dataframe)\n",
        "\n",
        "# Extract feature array with dimensions (n_samples, n_doppler_bins, n_time_bins, 1)\n",
        "feature_array = np.stack( dataset_dataframe['Spectrogram'].to_numpy() )\n",
        "n_samples, n_doppler_bins, n_time_bins = feature_array.shape\n",
        "\n",
        "# Collapse the feature array into a 1D vector, standardize the data and reshape back into spectrograms\n",
        "feature_array = feature_array.reshape(n_samples, n_doppler_bins * n_time_bins)\n",
        "feature_array = standardize_features(feature_array, training_indices)\n",
        "feature_array = feature_array.reshape( n_samples, n_doppler_bins, n_time_bins, 1)\n",
        "\n",
        "# labels are one-hot endcoded as required by Keras\n",
        "num_classes = 3\n",
        "labels = keras.utils.to_categorical(dataset_dataframe['Class index'].to_numpy(), num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl9Fw_kuXiyT"
      },
      "source": [
        "# Define model\n",
        "input_shape = (n_doppler_bins, n_time_bins, 1)\n",
        "\n",
        "device_indicator = '/device:GPU:0' # Change to '/device:cpu:0' to perform the computations on CPU\n",
        "with tf.device(device_indicator):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2000, activation='relu'))\n",
        "    model.add(Dense(2000, activation='relu'))\n",
        "    model.add(Dense(2000, activation='relu'))\n",
        "    model.add(Dense(2000, activation='relu'))\n",
        "    model.add(Dense(2000, activation='relu')) # Experiment with changing the number and size of the dense layers\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adam(),\n",
        "                metrics='categorical_accuracy')\n",
        "\n",
        "    batch_size = 16\n",
        "    epochs = 15\n",
        "    model.fit(feature_array[training_indices, :, :], labels[training_indices,:],\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(feature_array[test_indices, :, :], labels[test_indices,:]))\n",
        "\n",
        "    score = model.evaluate(feature_array[test_indices, :, :], labels[test_indices,:], verbose=0)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "print(f'The model has {model.count_params()} number of parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkwXQhyWxhSZ"
      },
      "source": [
        "## Exercise 1: Questions\n",
        "\n",
        "### 5.1\n",
        "Experiment with changing the number and the size of the Dense layers in the neural network. What happens if you include too many or too few?\n",
        "\n",
        "What happens if you select the size of one layer to 1? Why do you think that happens?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHz9hv67x-S5"
      },
      "source": [
        "\n",
        "Student's answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNygtv3hya5K"
      },
      "source": [
        "### 5.3\n",
        "Try to run the optimization on CPU. What changes? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcVl4S-Jy-PX"
      },
      "source": [
        "\n",
        "Student's answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEixrgA33xVM"
      },
      "source": [
        "<br> <br> <br> \n",
        "\n",
        "# Exercise 2: Convolutional neural networks\n",
        "\n",
        "Convolutional neural networks (CNNs) consists of a set of parametrized filters which stride across the input array/matrix. This technique has the following effect:\n",
        "\n",
        "\n",
        "*   We bias the network to extract features/patterns which are invariant to translations in the input data.\n",
        "*   The network is able to be more parameter-efficient as the parameters in a given layer are fit based on a larger propotion of the input data. \n",
        "\n",
        "\n",
        "This is in contrast to the fully connected neural network, which is not able to exploit the structure in the input data.\n",
        "\n",
        "A convolutional layer consisting of 32 individual filters of size 2,2 is initialized by the method:\n",
        "\n",
        "```\n",
        "model.add(Conv2D(32, (2, 2), activation='relu'))\n",
        "```\n",
        "The response from each filter is here passed through the activation function *relu* which simply truncates negative values to $0$ and is the identity function for non-negative values. For reasons related to numerical stability it is common that artificial neural networks do not have negative thoughts.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Hossam-H-Sultan/publication/333411007/figure/fig7/AS:766785846525952@1559827400204/ReLU-activation-function.png)\n",
        "\n",
        "Run the cell below to train a CNN to classify the micro-Doppler signatures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PlDMAvb88oh"
      },
      "source": [
        "# Define model\n",
        "input_shape = (n_doppler_bins, n_time_bins, 1)\n",
        "\n",
        "device_indicator = '/device:GPU:0' # Change to '/device:cpu:0' to remove GPU acceleration\n",
        "with tf.device(device_indicator):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (2, 2),\n",
        "                    activation='relu',\n",
        "                    input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(32, (2, 2), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                optimizer=keras.optimizers.Adam(),\n",
        "                metrics='categorical_accuracy')\n",
        "\n",
        "    batch_size = 16\n",
        "    epochs = 15\n",
        "    model.fit(feature_array[training_indices, :, :], labels[training_indices,:],\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(feature_array[test_indices, :, :], labels[test_indices,:]))\n",
        "\n",
        "    score = model.evaluate(feature_array[test_indices, :, :], labels[test_indices,:], verbose=0)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "print(f'The model has {model.count_params()} number of parameters')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuSdjJuN5QJg"
      },
      "source": [
        "## Exercise 2: Questions\n",
        "\n",
        "### 5.1\n",
        "Experiment with changing the number and size of the convolutional filters. What is the lowest number of model parameters you can achieve while retaining  high classification accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w0aYukQ5h0X"
      },
      "source": [
        "\n",
        "Student's answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Ces9OT52MT"
      },
      "source": [
        "<br> <br> <br> \n",
        "\n",
        "# Exercise 3: Principal Component Analasys (PCA)\n",
        "Another popular and effective approach to learn features from data is Principal Component Analysis (PCA).\n",
        "\n",
        "### Change of basis \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In the general case we have a dataset $X\\in R^{ n_{\\text{samples}}\\times n_{\\text{features}}}$.\n",
        "\n",
        "We are interested in reducing the dimensionality of the data, $n_{\\text{features}}$, while still retaining the structure in the data. If limit ourselves to a new basis which is a linear combination of the existing feature set we can describe the transformation as\n",
        "\n",
        "$\\tilde{X} = X P$, where the matrix $P\\in R^{n_\\text{features}, \\tilde{n}_\\text{features}}$ describes a linear transformation. \n",
        "\n",
        "The transformed data $\\tilde{X}\\in R^{ n_{\\text{samples}}\\times \\hat{n}_{\\text{features}}}$ has the reduced dimensionality $\\hat{n}_{\\text{features}} < n_{\\text{features}}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "### Basis of principle components\n",
        "\n",
        "But how do we choose an appropriate transformation $P$ ? \n",
        "\n",
        "PCA proposes that the covariance matrix of the features $\\Sigma$ completely describes the structure in the data and we should select a lower dimensional basis which is optimally able to reconstruct the covariance matrix. \n",
        "\n",
        "<br>\n",
        "\n",
        "The covariance matrix $\\Sigma$ is real and symmetric and so it is diagonalizable by the eigendecomposition,\n",
        "\n",
        "$\\Sigma = Q \\Lambda Q^T$, where $\\Lambda$ is a diagonal matrix of the eigenvalues of $\\Sigma$ and $Q$ is an orthonormal matrix where the columns are the eigenvectors of $\\Lambda$.\n",
        "\n",
        "<br>\n",
        "\n",
        "If we write out the matrix multiplication $ Q \\Lambda Q^T$ we find that if we remove eigenvectors with a small corresponding eigenvalue little information is lost. Therefore we select the change of bases $P$ as the $\\hat{n}_{\\text{features}}$ eigenvectors with the largest eigenvalues.\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\tilde{X} = X P$, where $ P = \\left[\\begin{array}{ccc}\n",
        "\\mid & & \\mid \\\\\n",
        "v_{1} & \\cdots & v_{\\hat{n}_{\\text{features}}} \\\\\n",
        "\\mid & & \\mid\n",
        "\\end{array}\\right]$ and $v$ denotes a eigenvector of $\\Sigma$ which are also called the principal components of the data $X$.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "### Reconstruction\n",
        "\n",
        "With the above technique we are able to reduce the dimensionality of the data. But how much information is lost? We can attempt to reconstruct the original data by the inverse transformation,\n",
        "\n",
        "$X = \\tilde{X}P^{-1} = \\tilde{X}P^{T}$.\n",
        "\n",
        "Run the cells below to plot the orignal data and reconstructed data for a given number of principal components.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-XF2VWmahfD"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def PCA_decomposition(spectrograms, PCA_COMPONENTS, training_indices):\n",
        "    pca = PCA(n_components = PCA_COMPONENTS)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    n_samples, n_doppler_bins, n_time_bins = spectrograms.shape\n",
        "    # Collapse the feature array into a 1D vector, standardize the data\n",
        "    feature_array = spectrograms.reshape(n_samples, n_doppler_bins * n_time_bins)\n",
        "\n",
        "    # fit the standardization and PCA transforms\n",
        "    scaler.fit(feature_array[training_indices,:])\n",
        "    spectrograms_standardized = scaler.transform(feature_array)\n",
        "    pca.fit(spectrograms_standardized[training_indices,:])\n",
        "\n",
        "    # Transform the data to its principal components\n",
        "    feature_array_PCA = pca.transform(spectrograms_standardized)\n",
        "    # Transform the data from principal components back to spectrograms\n",
        "    feature_array_standardized_filtererd = pca.inverse_transform(feature_array_PCA)\n",
        "    feature_array_filtererd = scaler.inverse_transform(feature_array_standardized_filtererd)\n",
        "\n",
        "    filtered_spectrograms = feature_array_filtererd.reshape(n_samples, n_doppler_bins, n_time_bins )\n",
        "\n",
        "    return feature_array_PCA, filtered_spectrograms, spectrograms_standardized, pca\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pLHfU6xo0dm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "I_SAMPLE = 1\n",
        "PCA_COMPONENTS = 9\n",
        "training_indices, test_indices = get_splits(dataset_dataframe)\n",
        "\n",
        "\n",
        "# Generate filtered spectrograms from PCA\n",
        "spectrograms = np.stack(dataset_dataframe['Spectrogram'].to_numpy())\n",
        "_, filtered_spectrograms, _, pca  = PCA_decomposition(spectrograms, PCA_COMPONENTS, training_indices)\n",
        "\n",
        "# Plot the spectrogram filtered by PCA and the unfiltered spectrogram\n",
        "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.suptitle(f\"Variance explained by {PCA_COMPONENTS} components is {round(sum(pca.explained_variance_ratio_)*100,1)}%. Class is {dataset_dataframe['Class'][I_SAMPLE]}\")\n",
        "ax1.imshow(filtered_spectrograms[I_SAMPLE,:,:], aspect='auto', cmap='jet', origin='lower', vmin=-40, interpolation='Nearest')\n",
        "ax2.imshow(spectrograms[I_SAMPLE,:,:], aspect='auto', cmap='jet', origin='lower', vmin=-40, interpolation='Nearest')\n",
        "\n",
        "ax1.set_ylabel('Doppler bin')\n",
        "ax2.set_ylabel('Doppler bin')\n",
        "ax1.set_xlabel('Time bin')\n",
        "ax2.set_xlabel('Time bin')\n",
        "ax2.set_title('Unfiltered spectrogram')\n",
        "ax1.set_title('PCA filtered spectrogram')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBfHz-uEWfZm"
      },
      "source": [
        "# Exercise 3: Questions\n",
        "### 3.1 \n",
        "Change the sample index *I_SAMPLE* and the number of principal components *PCA_COMPONENTS* in the above script. How many principal components is adequate to represent the micro-Doppler signatures? How many principal components do you need to differentiate the classes?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8BBBEt8YPZS"
      },
      "source": [
        "Student's answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeVEAVjFZHp1"
      },
      "source": [
        "<br> <br> <br>\n",
        "# Exercise 4: PCA and classification\n",
        "\n",
        "The reduced feature set in $\\tilde{X}$ can be used to classify the samples. \n",
        "\n",
        "We have defined a function PCA_decomposition() which returns $\\tilde{X}$.\n",
        "\n",
        "**Fit and evaluate the GaussianNaiveBayes classification model on the reduced feature set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeenwmnNsZjg"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB as GaussianNaiveBayes\n",
        "from sklearn.metrics import recall_score\n",
        "PCA_COMPONENTS = 10\n",
        "\n",
        "training_indices, test_indices = get_splits(dataset_dataframe)\n",
        "spectrograms = np.stack(dataset_dataframe['Spectrogram'].to_numpy())\n",
        "feature_array_PCA, _, _, _  = PCA_decomposition(spectrograms, PCA_COMPONENTS, training_indices)\n",
        "labels = dataset_dataframe['Class index'].to_numpy()\n",
        "\n",
        "model_NB = GaussianNaiveBayes()\n",
        "# Your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EDMmKjRbfh3"
      },
      "source": [
        "# Exercise 4: Questions\n",
        "### 4.1\n",
        "How many principal components do you need to reach high accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlbk9SvQb3Gg"
      },
      "source": [
        "Student's answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poFdiUegb3Tp"
      },
      "source": [
        "### 4.1\n",
        "What happens if you use too many or too few principal components? Why does this happen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gjW5Z6Fb3cl"
      },
      "source": [
        "Student's answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpWpk_6-Zp-5"
      },
      "source": [
        "\n",
        "<br> <br> <br>\n",
        "---\n",
        "\n",
        "\n",
        "Reference: \n",
        "\n",
        "A more detailed description and instructive visualizations of PCA can be found [here](https://arxiv.org/pdf/1404.1100.pdf) \n",
        "\n"
      ]
    }
  ]
}